**Общий Автоматизированный Процесс**

Система спроектирована для автоматической обработки документации из Git-репозитория. Общий процесс выглядит так:

1.  **Триггер:** Процесс запускается либо вручную для конкретного модуля (`npm run preprocess ...`), либо автоматически через webhook при коммите в репозиторий с исходными документами.
2.  **Получение Источников:** Система клонирует или обновляет локальную копию репозитория с документацией (`.md`, `.docx`) из Git (`source_md/`).
3.  **Предобработка (Скрипт `preprocess`):**
    *   Находятся все `.docx` и `.md` файлы для заданного модуля.
    *   `.docx` конвертируются в `.md` с помощью `pandoc`, извлекая медиафайлы.
    *   В `.md` файлах изображения (`<img>`, `![]()`) заменяются на Mermaid-диаграммы с помощью LLM (OpenRouter).
    *   Текст `.md` файлов разделяется на чанки по заголовкам и лимиту символов.
    *   *Промежуточный результат:* Набор `.md` файлов-чанков в папке `source_md/<module_id>/processed_chunks/`.
4.  **Основной Пайплайн (Скрипт `pipeline`):**
    *   Читаются `.md` файлы-чанки из `processed_chunks/`.
    *   Каждый чанк обрабатывается LLM (OpenRouter) для извлечения информации (например, генерация вопросов).
    *   Исходный текст чанка и извлеченная информация (вопросы) сохраняются в базу данных **PostgreSQL**.
    *   Извлеченные **вопросы** векторизуются с помощью внешнего сервиса (например, `vsegpt.ru`).
    *   Векторы **вопросов** вместе с метаданными (ID вопроса, ID чанка, ID модуля) загружаются в векторную базу данных **Qdrant**.
5.  **Результат:** Данные готовы для использования: текстовая информация и связи в PostgreSQL, векторы для семантического поиска вопросов в Qdrant.

------

**Описание Работы Скрипта Предобработки**

Исходя из структуры проекта и плана реализации (`implementation_plan.md`), скрипт предобработки (предположительно `scripts/preprocess.js` или аналогичный файл внутри `scripts/`) должен работать следующим образом:

1.  **Запуск:** Скрипт запускается командой вида `npm run preprocess -- --module=<id>`, где `<id>` - это идентификатор модуля, который нужно обработать.
2.  **Получение Исходных Файлов:**
    *   Сначала скрипт обращается к `src/utils/git_fetcher.js`, чтобы убедиться, что локальная копия репозитория с исходными `.md` и `.docx` файлами (находящаяся в папке `source_md/`) актуальна. Он либо клонирует репозиторий (если папки `source_md/` нет), либо обновляет его (`git pull`). Путь к репозиторию и данные для аутентификации берутся из файла `.env`.
    *   Далее скрипт ищет директорию, соответствующую указанному `module_id` внутри `source_md/`.
3.  **Поиск и Конвертация DOCX:**
    *   Внутри директории модуля скрипт находит все файлы с расширением `.docx`.
    *   Для каждого найденного `.docx` файла он вызывает утилиту `pandoc` (установленную в системе или Docker-контейнере) для конвертации его в формат Markdown (`.md`). Медиа-файлы (изображения) из `.docx` извлекаются в подпапку `media`.
4.  **Обработка Markdown Файлов:**
    *   Скрипт находит все `.md` файлы в директории модуля (включая только что сконвертированные).
    *   Для каждого `.md` файла выполняются следующие шаги:
        *   **Замена Изображений на Mermaid:** Файл парсится для поиска тегов изображений (`![]()` или `<img>`). Для каждого изображения делается запрос к API OpenRouter (используя ключ из `.env`) с промптом для генерации Mermaid-диаграммы, описывающей это изображение. Найденный тег изображения заменяется на блок кода Mermaid ```mermaid ... ```.
        *   **Разделение на Чанки:** Обработанный Markdown-контент разделяется на части (чанки).
            *   **Первичное разделение:** Происходит по заголовкам определенного уровня (например, `##`, уровень задается в `.env`).
            *   **Вторичное разделение (при необходимости):** Если чанк, полученный после разделения по заголовку, слишком длинный (превышает лимит символов, заданный в `.env`), он делится на более мелкие под-чанки по границам предложений или абзацев.
        *   **Сбор Метаданных:** Для каждого итогового чанка (или под-чанка) сохраняется его текст и метаданные: имя исходного файла, `module_id`, иерархия заголовков, порядковый номер чанка и, возможно, под-чанка.
5.  **Сохранение Результатов:**
    *   Все полученные чанки для указанного модуля собираются в единый массив.
    *   Этот массив сохраняется в виде JSON-файла в папку `output/`, например, `output/preprocessed_chunks_${moduleId}.json`.
6.  **Логирование:** На всех этапах работы скрипт выводит информационные сообщения и ошибки в консоль или лог-файл.

Таким образом, скрипт автоматизирует весь процесс подготовки текстовых данных из разных форматов к дальнейшей обработке LLM и загрузке в базы данных. 